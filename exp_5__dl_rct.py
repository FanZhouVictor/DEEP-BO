# -*- coding: utf-8 -*-
"""Exp 5__DL_Rct

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpIp6mxKq_N-fEVPwGEuG5YTRU8PzhCk

# Note: using dropout to prevent overfitting

# 1. enviroment setup
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

from google.colab import drive
drive.mount('/content/drive')

"""# 2. Data import"""

# Load dataset from CSV file
df = pd.read_excel('/content/drive/MyDrive/DL_ Rct/Data/After June 18th 2024/before feature selection/June-24th 2024-added glove brand feature .xlsx')

df.head()

df.info()

'''import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Calculate the correlation matrix
correlation_matrix = df.corr()

# Plot the correlation matrix using seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', vmin=-1, vmax=1, cbar=True, square=True, linewidths=.5)
plt.title('Correlation Matrix')
plt.show()
'''

"""### change the int to float"""

# Sample data loading
# df = pd.read_csv('your_dataset.csv')

# Converting 'section_feature' and 'Glove_type' to float64
df['section feature'] = df['section feature'].astype(float)
df['Glove type feature'] = df['Glove type feature'].astype(float)
df['Brand feature'] = df['Brand feature'].astype(float)

# Creating the new dataset
new_df = df.copy()

df = new_df
df.info()

"""##2.1 Check if there is missing data."""

df.isnull().sum()

"""# seperate data into training and testing based on brand feature."""

import os
import pandas as pd
from sklearn.preprocessing import RobustScaler


base_dir = '/content/drive/MyDrive/DL_ Rct/Data/DL_Exp4_data'

# Separate the Brand feature from the other features
brand_feature = df['Brand feature']
features_to_scale = df.drop(columns=['Brand feature'])

# Scale the features
scaler = RobustScaler()
scaled_features = scaler.fit_transform(features_to_scale)
scaled_features_df = pd.DataFrame(scaled_features, columns=features_to_scale.columns)


# Combine the scaled features with the Brand feature
scaled_df = pd.concat([scaled_features_df, brand_feature.reset_index(drop=True)], axis=1)


# Create the base directory if it doesn't exist
if not os.path.exists(base_dir):
    os.makedirs(base_dir)

# Iterate over each brand (1 to 7)
for brand in range(1, 8):
    # Create directories for each fold
    fold_dir = os.path.join(base_dir, f'fold{brand}_brand{brand}')
    if not os.path.exists(fold_dir):
        os.makedirs(fold_dir)

    # Separate the dataset into training and testing sets
    test_df = scaled_df[scaled_df['Brand feature'] == brand]
    train_df = scaled_df[scaled_df['Brand feature'] != brand]

    # Save the datasets into the corresponding folders
    test_df.to_csv(os.path.join(fold_dir, f'Fold{brand}_Brand{brand}_testing.csv'), index=False)
    train_df.to_csv(os.path.join(fold_dir, f'Fold{brand}_Brand{brand}_training.csv'), index=False)

print('Datasets have been successfully separated and saved.')

"""#2.2 Seperate the data to independent and dependent variables

##2.7 Custom dataset class
"""

import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, input, output):
        self.input = torch.tensor(input, dtype=torch.float32)
        self.output = torch.tensor(output, dtype=torch.float32)

    def __len__(self):
        return len(self.input)

    def __getitem__(self, idx):
        return self.input[idx], self.output[idx]
    def get_feature_names(self):
        return self.feature_names

"""### *** explain *** what is the functions within CustomDataset class.

##2.8 Create dataloader objects for training and test sets

#4. Run the model

##4.1 Check if Cuda is avaliable
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

"""##4.1 Set the seed for reproductivity"""

# Set the seed for reproducibility

import random

seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

"""##4.2 set model hyperparameters"""

input_dim=9
output_dim=1
hidden_dim=20 #yes
num_hidden_layers = 4 #yes
activation_name = 'Tanh' #yes
num_epochs = 2000 #yes
patience = 30  # Number of epochs to wait for improvement
best_loss = float('inf')
patience_counter = 0
weightdecay = 1E-6 #yes
LossFunction = nn.MSELoss()
LearningRate = 0.01 #yes
BatchSize=64 #yes

"""# 3. Model development

##3.1 Define the neural Network model
"""

def get_activation_function(name):
    activation_functions = {
        'ReLU': nn.ReLU(),
        'Sigmoid': nn.Sigmoid(),
        'Tanh': nn.Tanh(),
        'LeakyReLU': nn.LeakyReLU()
    }
    return activation_functions.get(name, nn.ReLU())  # Default to ReLU if the name is not found

class RegressionModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_hidden_layers, output_dim, activation_name):
        super(RegressionModel, self).__init__()
        self.hidden_layers = nn.ModuleList()

        # Input layer
        self.hidden_layers.append(nn.Linear(input_dim, hidden_dim))
        self.hidden_layers.append(get_activation_function(activation_name))

        # Hidden layers
        for _ in range(num_hidden_layers - 1):
            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))
            self.hidden_layers.append(get_activation_function(activation_name))

        # Output layer
        self.output_layer = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        for layer in self.hidden_layers:
            x = layer(x)
        x = self.output_layer(x)
        return x

"""#5. Training the model on Cuda, with earlystop"""

from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statistics import median, mean

# Initialize lists to store overall metrics
criterion = LossFunction

# Iterate through each fold directory
for fold in range(1, 8):
    fold_dir = os.path.join(base_dir, f'fold{fold}_brand{fold}')
    train_file = os.path.join(fold_dir, f'Fold{fold}_Brand{fold}_training.csv')
    test_file = os.path.join(fold_dir, f'Fold{fold}_Brand{fold}_testing.csv')

    # Load the training and testing data
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)

    X_train = train_df.drop(columns=['Real Rct', 'Brand feature']).values
    y_train = train_df['Real Rct'].values
    X_test = test_df.drop(columns=['Real Rct', 'Brand feature']).values
    y_test = test_df['Real Rct'].values

    # Convert to PyTorch tensors
    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))

    train_loader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BatchSize, shuffle=True)

    model = RegressionModel(input_dim, hidden_dim, num_hidden_layers, output_dim, activation_name).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LearningRate, weight_decay=weightdecay)

    train_losses = []
    test_losses = []

    # Reset early stopping variables for each fold
    best_loss = float('inf')
    patience_counter = 0


    # Training the model
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for train_inputs, train_targets in train_loader:
            train_inputs = train_inputs.to(device)
            train_targets = train_targets.to(device)
            optimizer.zero_grad()
            outputs = model(train_inputs)
            loss = criterion(outputs.squeeze(), train_targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        model.eval()
        test_loss = 0
        with torch.no_grad():
            for test_inputs, test_targets in test_loader:
                test_inputs = test_inputs.to(device)
                test_targets = test_targets.to(device)
                test_outputs = model(test_inputs)
                loss = criterion(test_outputs.squeeze(), test_targets)
                test_loss += loss.item()
        test_loss /= len(test_loader)
        test_losses.append(test_loss)

        if (epoch + 1) % 100 == 0:
            print(f'Fold {fold} - Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

        # Early stopping
        if test_loss < best_loss:
            best_loss = test_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch + 1}')
            break

    # Calculate metrics for the current fold
    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for test_inputs, test_targets in test_loader:
            test_inputs = test_inputs.to(device)
            test_targets = test_targets.to(device)
            test_outputs = model(test_inputs)
            y_true.extend(test_targets.cpu().numpy().reshape(-1))
            y_pred.extend(test_outputs.squeeze().cpu().numpy().reshape(-1))

    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    residuals = y_true - y_pred


    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    n = len(y_true)
    p = input_dim
    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if n > p + 1 else r2
    mae_score = mean_absolute_error(y_true, y_pred)
    mse_score = mean_squared_error(y_true, y_pred)

    print(f'Fold {fold} - R^2: {r2:.4f}, Adjusted R^2: {adjusted_r2:.4f}, MAE: {mae_score:.4f}, MSE: {mse_score:.4f}')
    # Statistical analysis of the target variable on training data
    print(f'Statistical Analysis of Training Data - Fold {fold}')
    print(f'Len: {len(y_train)}, Min: {min(y_train)}, Max: {max(y_train)}, Median: {median(y_train)}, Mean: {mean(y_train)}')

    # Statistical analysis of the target variable on testing data
    print(f'Statistical Analysis of Testing Data - Fold {fold}')
    print(f'Len: {len(y_test)}, Min: {min(y_test)}, Max: {max(y_test)}, Median: {median(y_test)}, Mean: {mean(y_test)}')


    # Plot loss curves, residuals, and loss residuals for the current fold horizontally
    fig, axs = plt.subplots(1, 3, figsize=(18, 6))

    # Plot loss curves
    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')
    axs[0].plot(range(1, len(test_losses) + 1), test_losses, label='Testing Loss')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].set_title(f'Training and Testing Loss - Fold {fold}')
    axs[0].legend()

    # Plot residuals
    axs[1].scatter(range(1, len(residuals) + 1), residuals, alpha=0.7, label=f'Residuals')
    axs[1].set_xlabel('Index')
    axs[1].set_ylabel('Residuals')
    axs[1].set_title(f'Residuals - Fold {fold}')
    axs[1].legend()
    axs[1].grid(True)

    # Plot loss residuals
    loss_residual = np.array(test_losses) - np.array(train_losses)
    axs[2].scatter(range(1, len(loss_residual) + 1), loss_residual, alpha=0.7, label=f'Loss Residuals')
    axs[2].set_xlabel('Epoch')
    axs[2].set_ylabel('Loss Residuals')
    axs[2].set_title(f'Loss Residuals - Fold {fold}')
    axs[2].legend()
    axs[2].grid(True)

    plt.tight_layout()
    plt.show()